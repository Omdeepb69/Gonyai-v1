{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gonyai-v1 — Extended Benchmark: 160M vs 1B+ Models\n",
    "### Investigating Specialization vs. Scale in Low-Resource Konkani NLP\n",
    "\n",
    "**Models Tested:**\n",
    "* **Specialized:** Gonyai-v1 (160M)\n",
    "* **Sub-1B Baselines:** SmolLM2 (360M), Qwen2.5 (0.5B)\n",
    "* **1B+ Challengers:** Gemma 3 (1B), TinyLlama (1.1B), Qwen2.5 (1.5B)\n",
    "\n",
    "**Author:** Omdeep (stud.odb1@gec.ac.in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, math, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "try:\n",
    "    from transformers import Gemma3ForCausalLM\n",
    "    GEMMA3_AVAILABLE = True\n",
    "except ImportError:\n",
    "    GEMMA3_AVAILABLE = False\n",
    "    print(\"⚠ Gemma3ForCausalLM not found — update transformers: pip install -q -U transformers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = [\n",
    "    {\"display\": \"Gonyai-v1\\n(160M)\", \"hf_id\": \"omdeep22/Gonyai-v1\", \"params_m\": 160, \"group\": \"specialized\", \"loader\": \"auto\"},\n",
    "    {\"display\": \"SmolLM2\\n(360M)\", \"hf_id\": \"HuggingFaceTB/SmolLM2-360M-Instruct\", \"params_m\": 360, \"group\": \"sub1b\", \"loader\": \"auto\"},\n",
    "    {\"display\": \"Qwen2.5\\n(0.5B)\", \"hf_id\": \"Qwen/Qwen2.5-0.5B-Instruct\", \"params_m\": 500, \"group\": \"sub1b\", \"loader\": \"auto\"},\n",
    "    {\"display\": \"Gemma3\\n(1B)\", \"hf_id\": \"google/gemma-3-1b-it\", \"params_m\": 1000, \"group\": \"1b_plus\", \"loader\": \"gemma3\"},\n",
    "    {\"display\": \"TinyLlama\\n(1.1B)\", \"hf_id\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", \"params_m\": 1100, \"group\": \"1b_plus\", \"loader\": \"auto\"},\n",
    "    {\"display\": \"Qwen2.5\\n(1.5B)\", \"hf_id\": \"Qwen/Qwen2.5-1.5B-Instruct\", \"params_m\": 1500, \"group\": \"1b_plus\", \"loader\": \"auto\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOMAINS = {\n",
    "    \"Poetic\": [\"सांजवेळार दर्यावेळेर बसतना मनाक वेगळीच शांती जाणवता.\", \"पावसाचे थेंब माटयेर पडटात तेन्ना रानाचो सुगंध पांगता.\", \"चंद्राच्या उजवाडांत न्हंयच्या उदकाचें प्रतिबिंब दिसता.\", \"फुलाचो रंग पळेतना काळजांत मोगाची लाट उठता.\"],\n",
    "    \"Conversational\": [\"आज जेवणाक कितें केलांस, भूक लागल्या.\", \"तूं कोठून आयलो, इतल्या उशिरान?\", \"बाजारांत गेल्लो तेन्ना तुजो भाव भेटलो.\", \"ये, बस, चहा पी, मागीर उलोव.\"],\n",
    "    \"Storytelling\": [\"एके सांजे म्हातारो दर्याकांठार बसून काणी सांगतालो.\", \"त्या गांवांत एक सोबीत चेडूं आशिल्ली.\", \"राजाच्या वाड्यांत अशें घडलें जें कोणें पळेल्लें नाशिल्लें.\", \"धुक्यान भरिल्ल्या वाटेर ल्हान भुरगो एकलोच भोंवतालो.\"],\n",
    "    \"Grammar/Prose\": [\"ताणें सांगिल्लें काम ताणेंच केल्लें नाशिल्ल्यान राग आयलो.\", \"जर तूं वेळार आयलो आसतो तर आमी एकठांय वचूं येतालें.\", \"ती रडपाक लागली कारण तिचो आवडटो फूल तोडिल्लो.\", \"भुरग्यांनी खेळ संपयलो आनी घरा परत येवन जेवण केलें.\"]\n",
    "}\n",
    "MARATHI_CONTROL = [\"संध्याकाळी समुद्रकिनारी बसताना मनाला एक वेगळीच शांती जाणवते.\", \"पावसाचे थेंब जमिनीवर पडतात तेव्हा मातीचा सुगंध सर्वत्र पसरतो.\"]\n",
    "HINDI_CONTROL = [\"शाम को समुद्र के किनारे बैठने पर मन को एक अलग ही शांति मिलती है।\", \"बारिश की बूंदें जब जमीन पर गिरती हैं तो मिट्टी की खुशबू फैल जाती है।\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def load_model_and_tokenizer(cfg):\n",
    "    hf_id, loader = cfg[\"hf_id\"], cfg[\"loader\"]\n",
    "    dtype = torch.bfloat16 if DEVICE == \"cuda\" else torch.float32\n",
    "    tokenizer = AutoTokenizer.from_pretrained(hf_id, trust_remote_code=True)\n",
    "    if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
    "    sgpu = {\"\": 0} if DEVICE == \"cuda\" else {\"\": \"cpu\"}\n",
    "    if loader == \"gemma3\" and GEMMA3_AVAILABLE:\n",
    "        model = Gemma3ForCausalLM.from_pretrained(hf_id, torch_dtype=dtype, device_map=sgpu).eval()\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(hf_id, trust_remote_code=True, torch_dtype=dtype, device_map=sgpu).eval()\n",
    "    return model, tokenizer\n",
    "\n",
    "def compute_ppl(model, tokenizer, sentences):\n",
    "    if not sentences: return None\n",
    "    t_nll, t_tok = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for s in sentences:\n",
    "            enc = tokenizer(s, return_tensors=\"pt\"); ids = enc[\"input_ids\"].to(DEVICE)\n",
    "            if ids.shape[1] < 2: continue\n",
    "            out = model(ids); logits = out.logits if hasattr(out, \"logits\") else out[0]\n",
    "            loss = torch.nn.functional.cross_entropy(logits[:, :-1, :].reshape(-1, logits.size(-1)), ids[:, 1:].reshape(-1), reduction=\"sum\")\n",
    "            t_nll += loss.item(); t_tok += ids[:, 1:].numel()\n",
    "    return math.exp(min(t_nll / t_tok, 20)) if t_tok > 0 else None\n",
    "\n",
    "def get_token_stats(tokenizer, sentences):\n",
    "    t_tok = sum(len(tokenizer.encode(s)) for s in sentences)\n",
    "    t_wrd = sum(len(s.split()) for s in sentences)\n",
    "    return t_tok, (t_tok / t_wrd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "base_tok_count = 0\n",
    "all_konkani = [s for sents in DOMAINS.values() for s in sents]\n",
    "\n",
    "for i, cfg in enumerate(MODELS):\n",
    "    model, tokenizer = load_model_and_tokenizer(cfg)\n",
    "    t_count, fertility = get_token_stats(tokenizer, all_konkani)\n",
    "    if i == 0: base_tok_count = t_count\n",
    "    \n",
    "    row = {\"Model\": cfg[\"display\"], \"Group\": cfg[\"group\"], \"Params_M\": cfg[\"params_m\"], \"Fertility\": fertility, \n",
    "           \"Token_Tax\": ((t_count / base_tok_count) - 1) * 100 if i > 0 else 0, \"Context\": 2048 / fertility}\n",
    "    \n",
    "    for d, s in DOMAINS.items(): row[f\"PPL {d}\"] = compute_ppl(model, tokenizer, s)\n",
    "    row[\"PPL Overall\"] = compute_ppl(model, tokenizer, all_konkani)\n",
    "    row[\"PPL Marathi\"] = compute_ppl(model, tokenizer, MARATHI_CONTROL)\n",
    "    row[\"PPL Hindi\"] = compute_ppl(model, tokenizer, HINDI_CONTROL)\n",
    "    \n",
    "    ov = row[\"PPL Overall\"]\n",
    "    row[\"Sanity\"] = \"PASS ✓\" if ov and ov < min(row[\"PPL Marathi\"], row[\"PPL Hindi\"]) else \"FAIL ✗\"\n",
    "    results.append(row)\n",
    "    del model; torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=(20, 18))\n",
    "colors = {\"specialized\": \"#E53935\", \"sub1b\": \"#1E88E5\", \"1b_plus\": \"#43A047\"}\n",
    "bar_clrs = [colors[r[\"Group\"]] for r in results]\n",
    "lbls = [r[\"Model\"].replace(\"\\n\", \" \") for r in results]\n",
    "\n",
    "def plot_bar(ax, key, title, is_pct=False):\n",
    "    vals = [r.get(key, 0) for r in results]\n",
    "    ax.bar(lbls, vals, color=bar_clrs, edgecolor=\"black\")\n",
    "    ax.set_title(title, fontweight=\"bold\")\n",
    "    for i, v in enumerate(vals): ax.text(i, v, f\"{v:.1f}%\" if is_pct else f\"{v:.1f}\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "plot_bar(axes[0,0], \"PPL Poetic\", \"Poetic PPL (↓)\")\n",
    "plot_bar(axes[0,1], \"PPL Conversational\", \"Conv PPL (↓)\")\n",
    "plot_bar(axes[0,2], \"PPL Overall\", \"Overall PPL (↓)\")\n",
    "plot_bar(axes[1,0], \"Fertility\", \"Fertility Rate (↓)\")\n",
    "plot_bar(axes[1,1], \"Token_Tax\", \"Token Tax vs Gonyai (% ↑)\", True)\n",
    "plot_bar(axes[1,2], \"Context\", \"Words per 2k Window (↑)\")\n",
    "\n",
    "# Sanity Logic for Panel 9\n",
    "ax9 = axes[2,2]; x = np.arange(len(lbls)); w = 0.25\n",
    "ax9.bar(x-w, [r[\"PPL Overall\"] for r in results], w, label=\"Konkani\", color=\"#E53935\")\n",
    "ax9.bar(x, [r[\"PPL Marathi\"] for r in results], w, label=\"Marathi\", color=\"#1E88E5\")\n",
    "ax9.bar(x+w, [r[\"PPL Hindi\"] for r in results], w, label=\"Hindi\", color=\"#43A047\")\n",
    "ax9.set_title(\"Cross-Lingual Sanity\"); ax9.legend()\n",
    "\n",
    "plt.tight_layout(); plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
