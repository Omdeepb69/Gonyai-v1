{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deef5f0c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import PreTrainedTokenizerFast, PretrainedConfig, PreTrainedModel\n",
    "from datasets import load_dataset\n",
    "\n",
    "# ==========================================\n",
    "# 1. ARCHITECTURE DEFINITION\n",
    "# ==========================================\n",
    "\n",
    "class KonkanSmallConfig(PretrainedConfig):\n",
    "    model_type = \"konkangpt\"\n",
    "    def __init__(self, vocab_size=32000, d_model=768, n_layers=12, n_heads=12, \n",
    "                 d_ff=3072, max_len=1024, dropout=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.n_layers = n_layers\n",
    "        self.n_heads = n_heads\n",
    "        self.d_ff = d_ff\n",
    "        self.max_len = max_len\n",
    "        self.dropout = dropout\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_seq_len=2048):\n",
    "        super().__init__()\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "    def forward(self, x, seq_len):\n",
    "        t = torch.arange(seq_len, device=x.device, dtype=self.inv_freq.dtype)\n",
    "        freqs = torch.outer(t, self.inv_freq)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        return emb.cos(), emb.sin()\n",
    "\n",
    "def rotate_half(x):\n",
    "    x1, x2 = x.chunk(2, dim=-1)\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "def apply_rotary_pos_emb(x, cos, sin):\n",
    "    cos = cos[:x.shape[-2], :].unsqueeze(0).unsqueeze(0)\n",
    "    sin = sin[:x.shape[-2], :].unsqueeze(0).unsqueeze(0)\n",
    "    return (x * cos) + (rotate_half(x) * sin)\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "    def forward(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) * self.weight\n",
    "\n",
    "class SwiGLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x, gate = x.chunk(2, dim=-1)\n",
    "        return F.silu(gate) * x\n",
    "\n",
    "class KonkanBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.n_heads = config.n_heads\n",
    "        self.head_dim = config.d_model // config.n_heads\n",
    "        self.q_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
    "        self.k_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
    "        self.v_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
    "        self.o_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
    "        self.gate_up_proj = nn.Linear(config.d_model, 2 * config.d_ff, bias=False)\n",
    "        self.down_proj = nn.Linear(config.d_ff, config.d_model, bias=False)\n",
    "        self.input_layernorm = RMSNorm(config.d_model)\n",
    "        self.post_attention_layernorm = RMSNorm(config.d_model)\n",
    "        self.act = SwiGLU()\n",
    "\n",
    "    def forward(self, x, cos, sin, mask):\n",
    "        residual = x\n",
    "        x = self.input_layernorm(x)\n",
    "        b, t, c = x.shape\n",
    "        q = self.q_proj(x).reshape(b, t, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_proj(x).reshape(b, t, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(x).reshape(b, t, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        q = apply_rotary_pos_emb(q, cos, sin)\n",
    "        k = apply_rotary_pos_emb(k, cos, sin)\n",
    "        y = F.scaled_dot_product_attention(q, k, v, attn_mask=mask)\n",
    "        y = y.transpose(1, 2).contiguous().reshape(b, t, c)\n",
    "        x = residual + self.o_proj(y)\n",
    "        x = x + self.down_proj(self.act(self.gate_up_proj(self.post_attention_layernorm(x))))\n",
    "        return x\n",
    "\n",
    "class KonkanGPT(PreTrainedModel):\n",
    "    config_class = KonkanSmallConfig\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.token_emb = nn.Embedding(config.vocab_size, config.d_model)\n",
    "        self.rope = RotaryEmbedding(config.d_model // config.n_heads, config.max_len)\n",
    "        self.layers = nn.ModuleList([KonkanBlock(config) for _ in range(config.n_layers)])\n",
    "        self.norm = RMSNorm(config.d_model)\n",
    "        self.head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
    "        self.token_emb.weight = self.head.weight\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(self, input_ids, labels=None, **kwargs):\n",
    "        b, t = input_ids.shape\n",
    "        cos, sin = self.rope(input_ids, t)\n",
    "        mask = torch.tril(torch.ones(t, t, device=input_ids.device)).view(1, 1, t, t).bool()\n",
    "        x = self.token_emb(input_ids)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, cos, sin, mask)\n",
    "        logits = self.head(self.norm(x))\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # FIX: Using .reshape() to avoid stride compatibility errors\n",
    "            loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), labels.reshape(-1))\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "# ==========================================\n",
    "# 2. CHECKPOINT & DATA MANAGER\n",
    "# ==========================================\n",
    "\n",
    "class PitstopManager:\n",
    "    def __init__(self, save_dir=\"pitstops\", max_to_keep=2):\n",
    "        self.save_dir = save_dir\n",
    "        self.max_to_keep = max_to_keep\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    def save(self, model, optimizer, scaler, epoch, step):\n",
    "        raw_model = getattr(model, \"_orig_mod\", model)\n",
    "        checkpoint = {\n",
    "            'epoch': epoch, 'step': step,\n",
    "            'model_state_dict': raw_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scaler_state_dict': scaler.state_dict(),\n",
    "        }\n",
    "        final_path = os.path.join(self.save_dir, f\"pitstop_step_{step}.pt\")\n",
    "        torch.save(checkpoint, final_path)\n",
    "        ckpts = sorted(glob.glob(os.path.join(self.save_dir, \"pitstop_step_*.pt\")), key=os.path.getmtime)\n",
    "        while len(ckpts) > self.max_to_keep:\n",
    "            os.remove(ckpts.pop(0))\n",
    "\n",
    "    def load_latest(self, model, optimizer, scaler):\n",
    "        ckpts = sorted(glob.glob(os.path.join(self.save_dir, \"pitstop_step_*.pt\")), key=os.path.getmtime)\n",
    "        if ckpts:\n",
    "            latest_path = ckpts[-1]\n",
    "            print(f\"üîÑ Resuming from: {latest_path}\")\n",
    "            ckpt = torch.load(latest_path, map_location=\"cpu\")\n",
    "            model.load_state_dict(ckpt['model_state_dict'])\n",
    "            optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
    "            scaler.load_state_dict(ckpt['scaler_state_dict'])\n",
    "            return ckpt['epoch'], ckpt['step']\n",
    "        return 0, 0\n",
    "\n",
    "def pack_dataset(tokenizer, data_path, max_seq_len=1024):\n",
    "    print(\"üì¶ Packing Dataset (Constant Length Training)...\")\n",
    "    ds = load_dataset(\"text\", data_files={\"train\": data_path}, split=\"train\")\n",
    "    tokenized = ds.map(lambda x: tokenizer(x[\"text\"]), batched=True, remove_columns=[\"text\"], num_proc=4)\n",
    "    all_ids = []\n",
    "    for ids in tokenized[\"input_ids\"]:\n",
    "        all_ids.extend(ids + [tokenizer.eos_token_id])\n",
    "    chunk_size = max_seq_len + 1\n",
    "    total_chunks = len(all_ids) // chunk_size\n",
    "    packed_data = torch.tensor(all_ids[:total_chunks * chunk_size]).reshape(total_chunks, chunk_size)\n",
    "    print(f\"‚úÖ Created {len(packed_data)} dense blocks.\")\n",
    "    return packed_data\n",
    "\n",
    "class PackedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tensor_data): self.data = tensor_data\n",
    "    def __len__(self): return len(self.data)\n",
    "    def __getitem__(self, i): return {\"input_ids\": self.data[i]}\n",
    "\n",
    "# ==========================================\n",
    "# 3. TRAINING ENGINE\n",
    "# ==========================================\n",
    "\n",
    "def train_pure_power():\n",
    "    device = \"cuda\"\n",
    "    TOKEN_DIR = \"konkani-tokenizer-v3-32k\"\n",
    "    DATA_PATH = \"/kaggle/input/konkani-book-corpus/konkani_book_corpus.txt\"\n",
    "    \n",
    "    tokenizer = PreTrainedTokenizerFast.from_pretrained(TOKEN_DIR)\n",
    "    model = KonkanGPT(KonkanSmallConfig(vocab_size=len(tokenizer))).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.1)\n",
    "    scaler = torch.amp.GradScaler('cuda')\n",
    "    pitstop_manager = PitstopManager(\"pitstops\")\n",
    "\n",
    "    # Load checkpoint if exists\n",
    "    start_epoch, start_step = pitstop_manager.load_latest(model, optimizer, scaler)\n",
    "\n",
    "    # DRY RUN\n",
    "    print(\"üß™ Dry Run...\")\n",
    "    test_ids = torch.randint(0, 100, (2, 129)).to(device)\n",
    "    with torch.no_grad():\n",
    "        _ = model(test_ids[:, :-1].contiguous(), labels=test_ids[:, 1:].contiguous())\n",
    "    print(\"‚úÖ Verified.\")\n",
    "\n",
    "    # Data\n",
    "    packed_tensor = pack_dataset(tokenizer, DATA_PATH)\n",
    "    train_loader = DataLoader(PackedDataset(packed_tensor), batch_size=4, shuffle=True)\n",
    "\n",
    "    model = torch.compile(model) \n",
    "    accum_steps = 16 \n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(start_epoch, 2):\n",
    "        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch}\")\n",
    "        for step, batch in progress_bar:\n",
    "            if epoch == start_epoch and step <= start_step: continue\n",
    "\n",
    "            ids = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "            inputs, labels = ids[:, :-1].contiguous(), ids[:, 1:].contiguous()\n",
    "\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                outputs = model(inputs, labels=labels)\n",
    "                loss = outputs[\"loss\"] / accum_steps\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            if (step + 1) % accum_steps == 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            if step % 10 == 0:\n",
    "                progress_bar.set_postfix({\"loss\": f\"{loss.item()*accum_steps:.4f}\"})\n",
    "            \n",
    "            # Save checkpoint every 500 steps\n",
    "            if step > 0 and step % 500 == 0:\n",
    "                pitstop_manager.save(model, optimizer, scaler, epoch, step)\n",
    "\n",
    "    torch.save(getattr(model, \"_orig_mod\", model).state_dict(), \"konkan_160m_final_pure.pt\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_pure_power()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a681d49f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# 1. SETUP - Match these to your training config\n",
    "MODEL_PATH = \"konkan_160m_final_pure.pt\" # Or your latest checkpoint\n",
    "TOKEN_DIR = \"konkani-tokenizer-v3-32k\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 2. LOAD TOKENIZER AND MODEL\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(TOKEN_DIR)\n",
    "\n",
    "# Reuse your architecture definition from the training script\n",
    "# Ensure KonkanGPT and KonkanSmallConfig classes are defined above this\n",
    "config = KonkanSmallConfig(vocab_size=len(tokenizer))\n",
    "model = KonkanGPT(config).to(DEVICE)\n",
    "\n",
    "# Load weights\n",
    "state_dict = torch.load(MODEL_PATH, map_location=DEVICE)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "\n",
    "print(\"‚úÖ Model Loaded. Ready to generate.\")\n",
    "\n",
    "def generate(prompt, max_new_tokens=50, temperature=0.8, top_k=50):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "            logits = outputs[\"logits\"][:, -1, :] / temperature\n",
    "            \n",
    "            # Optional: Top-K sampling to prevent \"junk\" tokens\n",
    "            if top_k > 0:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            \n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "            \n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "                \n",
    "    return tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# 3. TEST IT\n",
    "prompt = \"‡§ó‡•ã‡§Ç‡§Ø ‡§è‡§ï\" # \"Goa is a...\"\n",
    "print(f\"\\nPrompt: {prompt}\")\n",
    "print(f\"Generated: {generate(prompt)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64157a6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Try these parameters for more \"story-telling\" flow\n",
    "print(generate(\"‡§ó‡•ã‡§Ç‡§Ø ‡§è‡§ï\", max_new_tokens=100, temperature=0.85, top_k=40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d517639",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import PreTrainedTokenizerFast, PretrainedConfig, PreTrainedModel\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ==========================================\n",
    "# 1. YOUR MODEL ARCHITECTURE (Exact Copy)\n",
    "# ==========================================\n",
    "# We include this so the script can load your pre-trained weights correctly.\n",
    "\n",
    "class KonkanSmallConfig(PretrainedConfig):\n",
    "    model_type = \"konkangpt\"\n",
    "    def __init__(self, vocab_size=32000, d_model=768, n_layers=12, n_heads=12, \n",
    "                 d_ff=3072, max_len=1024, dropout=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.n_layers = n_layers\n",
    "        self.n_heads = n_heads\n",
    "        self.d_ff = d_ff\n",
    "        self.max_len = max_len\n",
    "        self.dropout = dropout\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_seq_len=2048):\n",
    "        super().__init__()\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "    def forward(self, x, seq_len):\n",
    "        t = torch.arange(seq_len, device=x.device, dtype=self.inv_freq.dtype)\n",
    "        freqs = torch.outer(t, self.inv_freq)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        return emb.cos(), emb.sin()\n",
    "\n",
    "def rotate_half(x):\n",
    "    x1, x2 = x.chunk(2, dim=-1)\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "def apply_rotary_pos_emb(x, cos, sin):\n",
    "    cos = cos[:x.shape[-2], :].unsqueeze(0).unsqueeze(0)\n",
    "    sin = sin[:x.shape[-2], :].unsqueeze(0).unsqueeze(0)\n",
    "    return (x * cos) + (rotate_half(x) * sin)\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "    def forward(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) * self.weight\n",
    "\n",
    "class SwiGLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x, gate = x.chunk(2, dim=-1)\n",
    "        return F.silu(gate) * x\n",
    "\n",
    "class KonkanBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.n_heads = config.n_heads\n",
    "        self.head_dim = config.d_model // config.n_heads\n",
    "        self.q_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
    "        self.k_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
    "        self.v_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
    "        self.o_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
    "        self.gate_up_proj = nn.Linear(config.d_model, 2 * config.d_ff, bias=False)\n",
    "        self.down_proj = nn.Linear(config.d_ff, config.d_model, bias=False)\n",
    "        self.input_layernorm = RMSNorm(config.d_model)\n",
    "        self.post_attention_layernorm = RMSNorm(config.d_model)\n",
    "        self.act = SwiGLU()\n",
    "\n",
    "    def forward(self, x, cos, sin, mask):\n",
    "        residual = x\n",
    "        x = self.input_layernorm(x)\n",
    "        b, t, c = x.shape\n",
    "        q = self.q_proj(x).reshape(b, t, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_proj(x).reshape(b, t, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(x).reshape(b, t, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        q = apply_rotary_pos_emb(q, cos, sin)\n",
    "        k = apply_rotary_pos_emb(k, cos, sin)\n",
    "        y = F.scaled_dot_product_attention(q, k, v, attn_mask=mask)\n",
    "        y = y.transpose(1, 2).contiguous().reshape(b, t, c)\n",
    "        x = residual + self.o_proj(y)\n",
    "        x = x + self.down_proj(self.act(self.gate_up_proj(self.post_attention_layernorm(x))))\n",
    "        return x\n",
    "\n",
    "class KonkanGPT(PreTrainedModel):\n",
    "    config_class = KonkanSmallConfig\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.token_emb = nn.Embedding(config.vocab_size, config.d_model)\n",
    "        self.rope = RotaryEmbedding(config.d_model // config.n_heads, config.max_len)\n",
    "        self.layers = nn.ModuleList([KonkanBlock(config) for _ in range(config.n_layers)])\n",
    "        self.norm = RMSNorm(config.d_model)\n",
    "        self.head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
    "        self.token_emb.weight = self.head.weight\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(self, input_ids, labels=None, **kwargs):\n",
    "        b, t = input_ids.shape\n",
    "        cos, sin = self.rope(input_ids, t)\n",
    "        mask = torch.tril(torch.ones(t, t, device=input_ids.device)).view(1, 1, t, t).bool()\n",
    "        x = self.token_emb(input_ids)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, cos, sin, mask)\n",
    "        logits = self.head(self.norm(x))\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            loss = F.cross_entropy(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "# ==========================================\n",
    "# 2. SFT DATASET & MASKING LOGIC\n",
    "# ==========================================\n",
    "\n",
    "class KonkanSFTDataset(Dataset):\n",
    "    def __init__(self, json_path, tokenizer, max_len=512):\n",
    "        self.data = []\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        # Load JSON\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            raw_data = json.load(f) # Assuming list of dicts\n",
    "            \n",
    "        print(f\"üîÑ Processing {len(raw_data)} SFT samples...\")\n",
    "        \n",
    "        # Pre-format logic\n",
    "        for item in raw_data:\n",
    "            instr = item['instruction'].strip()\n",
    "            resp = item['response'].strip()\n",
    "            \n",
    "            # 1. Format the string with clear delimiters\n",
    "            # The \\n are crucial for the model to learn structure\n",
    "            full_text = f\"<|user|>\\n{instr}\\n<|assistant|>\\n{resp}<|endoftext|>\"\n",
    "            \n",
    "            # 2. We need the length of the PROMPT ONLY to mask it later\n",
    "            prompt_text = f\"<|user|>\\n{instr}\\n<|assistant|>\\n\"\n",
    "            \n",
    "            self.data.append({\n",
    "                \"full_text\": full_text,\n",
    "                \"prompt_text\": prompt_text\n",
    "            })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "def sft_collate_fn(batch, tokenizer, device):\n",
    "    \"\"\"\n",
    "    This is where the magic happens.\n",
    "    We create 'labels' that match 'input_ids', but we set the \n",
    "    Instruction part to -100 so the model ignores it for loss calculation.\n",
    "    \"\"\"\n",
    "    input_ids_list = []\n",
    "    labels_list = []\n",
    "    max_batch_len = 0\n",
    "    \n",
    "    for item in batch:\n",
    "        # Tokenize prompt and full text\n",
    "        prompt_ids = tokenizer.encode(item['prompt_text'], add_special_tokens=False)\n",
    "        full_ids = tokenizer.encode(item['full_text'], add_special_tokens=False)\n",
    "        \n",
    "        # Check length\n",
    "        if len(full_ids) > 1024:\n",
    "            full_ids = full_ids[:1024]\n",
    "            \n",
    "        # Create Label Mask\n",
    "        # Copy full_ids to labels\n",
    "        labels = list(full_ids)\n",
    "        \n",
    "        # Set the prompt part to -100 (Ignore Index)\n",
    "        # We assume prompt_ids is a prefix of full_ids\n",
    "        prompt_len = len(prompt_ids)\n",
    "        if prompt_len < len(labels):\n",
    "            for i in range(prompt_len):\n",
    "                labels[i] = -100\n",
    "        \n",
    "        input_ids_list.append(torch.tensor(full_ids))\n",
    "        labels_list.append(torch.tensor(labels))\n",
    "        max_batch_len = max(max_batch_len, len(full_ids))\n",
    "\n",
    "    # Pad everything to the right\n",
    "    padded_inputs = torch.full((len(batch), max_batch_len), tokenizer.pad_token_id, dtype=torch.long)\n",
    "    padded_labels = torch.full((len(batch), max_batch_len), -100, dtype=torch.long) # Pad labels with -100\n",
    "    \n",
    "    for i, (ids, labs) in enumerate(zip(input_ids_list, labels_list)):\n",
    "        l = len(ids)\n",
    "        padded_inputs[i, :l] = ids\n",
    "        padded_labels[i, :l] = labs\n",
    "        \n",
    "    return padded_inputs.to(device), padded_labels.to(device)\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 3. TRAINING LOOP\n",
    "# ==========================================\n",
    "\n",
    "def train_sft():\n",
    "    # SETUP\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    TOKEN_DIR = \"konkani-tokenizer-v3-32k\" # Path to your tokenizer folder\n",
    "    PRETRAINED_MODEL_PATH = \"konkan_160m_final_pure.pt\" # Your Pre-trained weights\n",
    "    JSON_PATH = \"/kaggle/input/sft-160m/sft_dataset_ds.json\"\n",
    "    \n",
    "    # 1. Load Tokenizer\n",
    "    tokenizer = PreTrainedTokenizerFast.from_pretrained(TOKEN_DIR)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "    # 2. Load Model Structure\n",
    "    config = KonkanSmallConfig(vocab_size=len(tokenizer))\n",
    "    model = KonkanGPT(config).to(device)\n",
    "    \n",
    "    # 3. Load Pre-trained Weights (Crucial for not forgetting)\n",
    "    if os.path.exists(PRETRAINED_MODEL_PATH):\n",
    "        print(f\"üì• Loading Pre-trained Weights from {PRETRAINED_MODEL_PATH}...\")\n",
    "        state_dict = torch.load(PRETRAINED_MODEL_PATH, map_location=device)\n",
    "        model.load_state_dict(state_dict, strict=False) # strict=False allows small mismatches safely\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è WARNING: Pre-trained weights not found! Training from scratch (Not Recommended for SFT).\")\n",
    "\n",
    "    # 4. Optimizer - LOW LR to prevent forgetting\n",
    "    # We use 1e-5 (very small) so we don't break the grammar knowledge\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "    \n",
    "    # 5. Dataset\n",
    "    dataset = KonkanSFTDataset(JSON_PATH, tokenizer)\n",
    "    loader = DataLoader(dataset, batch_size=4, shuffle=True, \n",
    "                        collate_fn=lambda b: sft_collate_fn(b, tokenizer, device))\n",
    "    \n",
    "    # 6. Training Loop\n",
    "    model.train()\n",
    "    EPOCHS = 3 # Small dataset, don't overdo it\n",
    "    ACCUM_STEPS = 4\n",
    "    \n",
    "    print(\"üöÄ Starting SFT Training (Masked Instruction Loss)...\")\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "        \n",
    "        for step, (inputs, labels) in enumerate(progress_bar):\n",
    "            \n",
    "            # Forward Pass\n",
    "            # Inputs = Full Text\n",
    "            # Labels = Full Text (but Instruction is -100)\n",
    "            outputs = model(inputs, labels=labels)\n",
    "            loss = outputs[\"loss\"] / ACCUM_STEPS\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            if (step + 1) % ACCUM_STEPS == 0:\n",
    "                # Gradient Clipping prevents \"exploding\" updates that ruin memory\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            total_loss += loss.item() * ACCUM_STEPS\n",
    "            progress_bar.set_postfix({\"loss\": f\"{loss.item() * ACCUM_STEPS:.4f}\"})\n",
    "            \n",
    "    # 7. Save SFT Model\n",
    "    save_path = \"konkan_sft_gonyai.pt\"\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"‚úÖ SFT Complete. Model saved to {save_path}\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. INFERENCE FUNCTION (TESTING)\n",
    "# ==========================================\n",
    "def chat_with_gonyai(instruction):\n",
    "    device = \"cuda\"\n",
    "    tokenizer = PreTrainedTokenizerFast.from_pretrained(\"konkani-tokenizer-v3-32k\")\n",
    "    config = KonkanSmallConfig(vocab_size=len(tokenizer))\n",
    "    model = KonkanGPT(config).to(device)\n",
    "    model.load_state_dict(torch.load(\"konkan_sft_gonyai.pt\"))\n",
    "    model.eval()\n",
    "    \n",
    "    # Format exactly like training data, but stop at assistant tag\n",
    "    prompt = f\"<|user|>\\n{instruction}\\n<|assistant|>\\n\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids, \n",
    "            max_new_tokens=150, \n",
    "            temperature=0.7, \n",
    "            top_p=0.9, \n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    # Extract only the response part\n",
    "    response = decoded.split(\"<|assistant|>\")[-1].strip()\n",
    "    return response\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_sft()\n",
    "    \n",
    "    # Test it immediately\n",
    "    print(\"\\nüß™ Testing Gonyai:\")\n",
    "    print(chat_with_gonyai(\"‡§§‡•Ç‡§Ç ‡§ï‡•ã‡§£?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a66c27",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# 1. ROBUST IMPORTS FOR BASE CLASSES\n",
    "try:\n",
    "    from transformers import PreTrainedModel, PretrainedConfig\n",
    "except ImportError:\n",
    "    # Fallback for older/specific versions\n",
    "    from transformers import PreTrainedModel, PreTrainedConfig as PretrainedConfig\n",
    "\n",
    "# 2. ARCHITECTURE DEFINITION\n",
    "class KonkanSmallConfig(PretrainedConfig):\n",
    "    model_type = \"konkangpt\"\n",
    "    def __init__(self, vocab_size=32000, d_model=768, n_layers=12, n_heads=12, \n",
    "                 d_ff=3072, max_len=1024, dropout=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.n_layers = n_layers\n",
    "        self.n_heads = n_heads\n",
    "        self.d_ff = d_ff\n",
    "        self.max_len = max_len\n",
    "        self.dropout = dropout\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_seq_len=2048):\n",
    "        super().__init__()\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "    def forward(self, x, seq_len):\n",
    "        t = torch.arange(seq_len, device=x.device, dtype=self.inv_freq.dtype)\n",
    "        freqs = torch.outer(t, self.inv_freq)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        return emb.cos(), emb.sin()\n",
    "\n",
    "def rotate_half(x):\n",
    "    x1, x2 = x.chunk(2, dim=-1)\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "def apply_rotary_pos_emb(x, cos, sin):\n",
    "    cos = cos[:x.shape[-2], :].unsqueeze(0).unsqueeze(0)\n",
    "    sin = sin[:x.shape[-2], :].unsqueeze(0).unsqueeze(0)\n",
    "    return (x * cos) + (rotate_half(x) * sin)\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "    def forward(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) * self.weight\n",
    "\n",
    "class SwiGLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x, gate = x.chunk(2, dim=-1)\n",
    "        return F.silu(gate) * x\n",
    "\n",
    "class KonkanBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.n_heads = config.n_heads\n",
    "        self.head_dim = config.d_model // config.n_heads\n",
    "        self.q_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
    "        self.k_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
    "        self.v_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
    "        self.o_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
    "        self.gate_up_proj = nn.Linear(config.d_model, 2 * config.d_ff, bias=False)\n",
    "        self.down_proj = nn.Linear(config.d_ff, config.d_model, bias=False)\n",
    "        self.input_layernorm = RMSNorm(config.d_model)\n",
    "        self.post_attention_layernorm = RMSNorm(config.d_model)\n",
    "        self.act = SwiGLU()\n",
    "\n",
    "    def forward(self, x, cos, sin, mask):\n",
    "        residual = x\n",
    "        x = self.input_layernorm(x)\n",
    "        b, t, c = x.shape\n",
    "        q = self.q_proj(x).reshape(b, t, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_proj(x).reshape(b, t, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(x).reshape(b, t, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        q = apply_rotary_pos_emb(q, cos, sin)\n",
    "        k = apply_rotary_pos_emb(k, cos, sin)\n",
    "        y = F.scaled_dot_product_attention(q, k, v, attn_mask=mask)\n",
    "        y = y.transpose(1, 2).contiguous().reshape(b, t, c)\n",
    "        x = residual + self.o_proj(y)\n",
    "        x = x + self.down_proj(self.act(self.gate_up_proj(self.post_attention_layernorm(x))))\n",
    "        return x\n",
    "\n",
    "class KonkanGPT(PreTrainedModel):\n",
    "    config_class = KonkanSmallConfig\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.token_emb = nn.Embedding(config.vocab_size, config.d_model)\n",
    "        self.rope = RotaryEmbedding(config.d_model // config.n_heads, config.max_len)\n",
    "        self.layers = nn.ModuleList([KonkanBlock(config) for _ in range(config.n_layers)])\n",
    "        self.norm = RMSNorm(config.d_model)\n",
    "        self.head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
    "        self.token_emb.weight = self.head.weight\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(self, input_ids, labels=None, **kwargs):\n",
    "        b, t = input_ids.shape\n",
    "        cos, sin = self.rope(input_ids, t)\n",
    "        mask = torch.tril(torch.ones(t, t, device=input_ids.device)).view(1, 1, t, t).bool()\n",
    "        x = self.token_emb(input_ids)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, cos, sin, mask)\n",
    "        logits = self.head(self.norm(x))\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            loss = F.cross_entropy(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "# 3. HELPER FUNCTIONS\n",
    "def load_gonyai(model_path, tokenizer_dir):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    tokenizer = PreTrainedTokenizerFast.from_pretrained(tokenizer_dir)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    config = KonkanSmallConfig(vocab_size=len(tokenizer))\n",
    "    model = KonkanGPT(config).to(device)\n",
    "    \n",
    "    print(f\"üì• Loading Gonyai Weights from {model_path}...\")\n",
    "    state_dict = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    return model, tokenizer\n",
    "\n",
    "def generate_manual(model, tokenizer, prompt, max_new_tokens=150, temperature=0.7):\n",
    "    device = next(model.parameters()).device\n",
    "    full_prompt = f\"<|user|>\\n{prompt}\\n<|assistant|>\\n\"\n",
    "    input_ids = tokenizer.encode(full_prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    generated = input_ids\n",
    "    for _ in range(max_new_tokens):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(generated)\n",
    "            next_token_logits = outputs[\"logits\"][:, -1, :]\n",
    "            next_token_logits = next_token_logits / temperature\n",
    "            probs = F.softmax(next_token_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            generated = torch.cat((generated, next_token), dim=1)\n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "    \n",
    "    input_length = input_ids.shape[1]\n",
    "    response_tokens = generated[0][input_length:]\n",
    "    return tokenizer.decode(response_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "# 4. EXECUTION\n",
    "TOKEN_DIR = \"konkani-tokenizer-v3-32k\"\n",
    "SFT_MODEL_PATH = \"konkan_sft_gonyai.pt\"\n",
    "\n",
    "# Load the model\n",
    "model, tokenizer = load_gonyai(SFT_MODEL_PATH, TOKEN_DIR)\n",
    "\n",
    "# Run the tests\n",
    "test_questions = [\n",
    "    \"‡§§‡•Å‡§ú‡•Ä ‡§µ‡§≥‡§ñ ‡§∏‡§æ‡§Ç‡§ó ‡§Ü‡§®‡•Ä ‡§§‡•Å‡§ú‡•á‡§Ç ‡§ß‡•ç‡§Ø‡•á‡§Ø ‡§ï‡§ø‡§§‡•á‡§Ç?\", \n",
    "    \"‡§ó‡•ã‡§Ç‡§Ø‡§ö‡•ç‡§Ø‡§æ ‡§∂‡§ø‡§ó‡§Æ‡•ã ‡§â‡§§‡•ç‡§∏‡§µ‡§æ‡§µ‡§ø‡§∂‡•Ä‡§Ç ‡§Æ‡§æ‡§π‡§ø‡§§‡•Ä ‡§¶‡•Ä.\",\n",
    "    \"‡§ö‡§µ‡§• ‡§∏‡§£‡§æ‡§ö‡•á‡§Ç ‡§Æ‡•ç‡§π‡§§‡•ç‡§µ ‡§∏‡§æ‡§Ç‡§ó.\",\n",
    "    \"‡§™‡§æ‡§µ‡§∏‡§æ‡§ö‡•á‡§∞ ‡§è‡§ï ‡§∏‡•ã‡§¨‡•Ä‡§§ ‡§ï‡§µ‡§ø‡§§‡§æ ‡§¨‡§∞‡•ã‡§µ.\",\n",
    "    \"‡§è‡§ï ‡§≤‡•ç‡§π‡§æ‡§® ‡§ï‡§æ‡§£‡•Ä ‡§∏‡§æ‡§Ç‡§ó ‡§ú‡§æ‡§§‡•Ç‡§Ç‡§§ ‡§è‡§ï ‡§∏‡§∏‡§£‡•ã ‡§Ü‡§®‡•Ä ‡§ï‡§æ‡§Ç‡§∏‡§µ ‡§Ü‡§∏‡§æ.\",\n",
    "    \"‡§ó‡•ã‡§Ç‡§Ø‡§ö‡•á ‡§™‡§Ø‡§≤‡•á ‡§Æ‡•Å‡§ñ‡•ç‡§Ø‡§Æ‡§Ç‡§§‡•ç‡§∞‡•Ä ‡§ï‡•ã‡§£ ‡§Ü‡§∂‡§ø‡§≤‡•ç‡§≤‡•á?\",\n",
    "    \"‡§Æ‡§æ‡§Ç‡§°‡§µ‡•Ä ‡§®‡•ç‡§π‡§Ç‡§Ø‡§ö‡•á‡§Ç ‡§ó‡•ã‡§Ç‡§Ø‡§ö‡•ç‡§Ø‡§æ ‡§ú‡•Ä‡§µ‡§ø‡§§‡§æ‡§§‡§≤‡•á‡§Ç ‡§∏‡•ç‡§•‡§æ‡§® ‡§∏‡§æ‡§Ç‡§ó.\",\n",
    "    \"‡§ï‡•ã‡§Ç‡§ï‡§£‡•Ä ‡§≠‡§æ‡§∂‡•á‡§Ç‡§§ '‡§∏‡§¶‡§æ‡§ö‡§æ‡§∞' ‡§Æ‡•ç‡§π‡§≥‡•ç‡§Ø‡§æ‡§∞ ‡§ï‡§ø‡§§‡•á‡§Ç?\",\n",
    "    \"Tell me a story about a king in English.\",\n",
    "    \"How can I learn Konkani fast?\"\n",
    "]\n",
    "\n",
    "print(\"\\nüé® Gonyai Multi-Directional Testing Starting...\\n\")\n",
    "for q in test_questions:\n",
    "    print(f\"User Question: {q}\")\n",
    "    response = generate_manual(model, tokenizer, q, temperature=0.7)\n",
    "    print(f\"Gonyai: {response}\")\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
