{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9b12bc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders, processors\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Path to your massive 5 million line corpus\n",
    "DATA_PATH = \"/kaggle/input/konkani-book-corpus/konkani_book_corpus.txt\"  \n",
    "SAVE_DIR = \"konkani-tokenizer-v3-32k\"\n",
    "VOCAB_SIZE = 32000  # Optimized for 160M parameter models\n",
    "\n",
    "def train_konkani_tokenizer():\n",
    "    if not os.path.exists(SAVE_DIR):\n",
    "        os.makedirs(SAVE_DIR)\n",
    "\n",
    "    print(f\"ðŸš€ Initializing BPE Tokenizer for {VOCAB_SIZE} vocab size...\")\n",
    "\n",
    "    # 1. Initialize BPE Model\n",
    "    # We use [UNK] for any characters not found in the vocab (rare emojis, etc.)\n",
    "    tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n",
    "\n",
    "    # 2. Pre-tokenizer\n",
    "    # ByteLevel is crucial for GPT models. It ensures we can encode any UTF-8 string,\n",
    "    # which is vital for Konkani's Devanagari script.\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "\n",
    "    # 3. Trainer Configuration\n",
    "    # We add specific special tokens that will be used in Phase 2 (Instruction Tuning)\n",
    "    trainer = trainers.BpeTrainer(\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        special_tokens=[\n",
    "            \"<s>\",      # Beginning of Sequence\n",
    "            \"<pad>\",    # Padding\n",
    "            \"</s>\",     # End of Sequence\n",
    "            \"[UNK]\",    # Unknown\n",
    "            \"[INST]\",   # User Instruction Start\n",
    "            \"[/INST]\"   # User Instruction End\n",
    "        ],\n",
    "        # initial_alphabet ensures all base characters are included before merging\n",
    "        initial_alphabet=pre_tokenizers.ByteLevel.alphabet()\n",
    "    )\n",
    "\n",
    "    # 4. Train the Tokenizer\n",
    "    # This might take a few minutes on 5M lines\n",
    "    print(f\"ðŸ”¥ Training started on {DATA_PATH}...\")\n",
    "    tokenizer.train(files=[DATA_PATH], trainer=trainer)\n",
    "\n",
    "    # 5. Post-Processing & Decoding\n",
    "    # This automatically handles the adding/stripping of spaces and special tokens\n",
    "    tokenizer.post_processor = processors.ByteLevel(trim_offsets=True)\n",
    "    tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "    # 6. Save as Hugging Face \"Fast\" Tokenizer\n",
    "    # This wrapper allows it to be loaded easily with AutoTokenizer or PreTrainedTokenizerFast\n",
    "    fast_tokenizer = PreTrainedTokenizerFast(\n",
    "        tokenizer_object=tokenizer,\n",
    "        bos_token=\"<s>\",\n",
    "        eos_token=\"</s>\",\n",
    "        pad_token=\"<pad>\",\n",
    "        unk_token=\"[UNK]\",\n",
    "    )\n",
    "\n",
    "    fast_tokenizer.save_pretrained(SAVE_DIR)\n",
    "    print(f\"âœ… Success! Tokenizer saved to: {SAVE_DIR}\")\n",
    "    print(f\"ðŸ“Š Final Vocab Size: {len(fast_tokenizer)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_konkani_tokenizer()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
